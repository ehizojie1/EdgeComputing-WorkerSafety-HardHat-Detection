{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Edge Computing Worker Safety Hard Hat Detection \u00b6 Protect workers with a TensorFlow Hard Hat object detection model running on a Jetson Nano Overview \u00b6 In this code pattern, we will describe how to implement a workplace safety use case using open source edge computing technologies and IBM's management architecture for deploying AI workloads to edge devices. In many factory environments, when employees enter a designated area, they must be wearing proper Personal Protective Equipment (PPE) such as a hard hat. This pattern demonstrates a solution which monitors the designated area and issues an alert only when an employee has been detected and is not wearing a hard hat. To reduce load on the network, the video stream object detection will be performed on edge devices managed by Open Horizon and IBM Edge Application Manager. To implement the architecture, the following components are required: Models need to be trained to identify a person wearing a hard hat. This is accomplished using IBM Cloud Annotations and Watson Machine Learning. The models need to be containerized and deployed to the edge. This is accomplished using Open Horizon Models need to be deployed to the camera to identify a human which will trigger the camera to start streaming. This is done using IBM Edge Application Manager . Architecture diagram \u00b6 The following diagram shows the workflow for the workplace safety hard hat detection system. Learning Objectives: \u00b6 In this code pattern you will learn how to: Part 1 \u00b6 Label a dataset of hard hat images with White / Blue / Yellow / Person objects using Cloud Annotations Create an instance of Cloud Object Storage (COS) in IBM Cloud Upload the hard hat dataset to COS Create a Watson Machine Learning instance Install Cloud Annotations command line interface - cacli Train a Tensorflow model using Watson Machine Learning Download the model with the cacli git clone object-detection-react Test model on your laptop Part 2 \u00b6 Set up a Jetson Nano with Ubuntu Attach a USB webcam and use a 5V barrel power supply to provide sufficient power Download the hard hat model with the cacli git clone object-detection-react Test model on the Jetson Part 3 \u00b6 Set up a laptop with HZN Exchange Hub Services Install docker on your Jetson Nano Install Open Horizon HZN anax client Register the Jetson into the Hub using the Open Horizon HZN anax client Part 4 \u00b6 Build a docker image which contains the model Register the workload pattern on the Exchange server Deploy the pattern to the edge device Prerequisites \u00b6 This tutorial can be completed using an IBM Cloud Lite account. Create an IBM Cloud account Log into IBM Cloud Nvidia Jetson Nano If you do not have a Jetson Nano or Jetson TX2, you can still complete Sections 1,2,3 Part 1 - Build a Hard Hat Object Detection Model \u00b6 Section 1 - Label a dataset of hard hat images with White / Blue / Yellow / Person objects \u00b6 This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects Instructions : Label Hard Hat Data Section 2 - Create IBM Cloud resources \u00b6 This section shows you how to create an instance of Cloud Object Storage (COS) in IBM Cloud. Create a COS bucket and upload the hard hat dataset to COS. Finally create Watson Machine Learning instance. Instructions : Create IBM Cloud Resources Section 3 - Train and Test a Hard Hat Detection Model \u00b6 This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop. Instructions : Train your Hard Hat model Part 2 - Configure a Jetson Nano \u00b6 Section 4 - Jetson Nano setup \u00b6 This section shows you how to configure a Jetson Nano to run object detection. Instructions : Configure Nvidia Jetson Nano Section 5 - Deploy and Test Hard Hat Object Detection \u00b6 This section shows you how to download and test the hard hat model. Instructions : Deploy / Test Model on the Jetson Part 3 - Open Horizon \u00b6 Section 6 - Open Horizon Exchange Hub \u00b6 This section guides you through the set up of a laptop with HZN Exchange Hub Services Instructions : Open Horizon Exchange Hub Services Section 7 - Open Horizon client setup \u00b6 Learn how to install Open Horizon HZN anax client and register the Jetson HZN anax client into your hub. Instructions : Open Horizon agent on the Jetson Part 4 - \u00b6 Section 8 - Containizer your model in a Docker image \u00b6 This section shows you how to build a docker image with a model Instructions : Build a Docker Image Section 9 - Register and deploy the Pattern to the edge \u00b6 This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device. Instructions : Deploy Horizon Pattern Enjoy! Give me feedback if you have suggestions on how to improve this workshop. License \u00b6 This workshop and code examples are licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"Introduction"},{"location":"index.html#edge-computing-worker-safety-hard-hat-detection","text":"Protect workers with a TensorFlow Hard Hat object detection model running on a Jetson Nano","title":"Edge Computing Worker Safety Hard Hat Detection"},{"location":"index.html#overview","text":"In this code pattern, we will describe how to implement a workplace safety use case using open source edge computing technologies and IBM's management architecture for deploying AI workloads to edge devices. In many factory environments, when employees enter a designated area, they must be wearing proper Personal Protective Equipment (PPE) such as a hard hat. This pattern demonstrates a solution which monitors the designated area and issues an alert only when an employee has been detected and is not wearing a hard hat. To reduce load on the network, the video stream object detection will be performed on edge devices managed by Open Horizon and IBM Edge Application Manager. To implement the architecture, the following components are required: Models need to be trained to identify a person wearing a hard hat. This is accomplished using IBM Cloud Annotations and Watson Machine Learning. The models need to be containerized and deployed to the edge. This is accomplished using Open Horizon Models need to be deployed to the camera to identify a human which will trigger the camera to start streaming. This is done using IBM Edge Application Manager .","title":"Overview"},{"location":"index.html#architecture-diagram","text":"The following diagram shows the workflow for the workplace safety hard hat detection system.","title":"Architecture diagram"},{"location":"index.html#learning-objectives","text":"In this code pattern you will learn how to:","title":"Learning Objectives:"},{"location":"index.html#part-1","text":"Label a dataset of hard hat images with White / Blue / Yellow / Person objects using Cloud Annotations Create an instance of Cloud Object Storage (COS) in IBM Cloud Upload the hard hat dataset to COS Create a Watson Machine Learning instance Install Cloud Annotations command line interface - cacli Train a Tensorflow model using Watson Machine Learning Download the model with the cacli git clone object-detection-react Test model on your laptop","title":"Part 1"},{"location":"index.html#part-2","text":"Set up a Jetson Nano with Ubuntu Attach a USB webcam and use a 5V barrel power supply to provide sufficient power Download the hard hat model with the cacli git clone object-detection-react Test model on the Jetson","title":"Part 2"},{"location":"index.html#part-3","text":"Set up a laptop with HZN Exchange Hub Services Install docker on your Jetson Nano Install Open Horizon HZN anax client Register the Jetson into the Hub using the Open Horizon HZN anax client","title":"Part 3"},{"location":"index.html#part-4","text":"Build a docker image which contains the model Register the workload pattern on the Exchange server Deploy the pattern to the edge device","title":"Part 4"},{"location":"index.html#prerequisites","text":"This tutorial can be completed using an IBM Cloud Lite account. Create an IBM Cloud account Log into IBM Cloud Nvidia Jetson Nano If you do not have a Jetson Nano or Jetson TX2, you can still complete Sections 1,2,3","title":"Prerequisites"},{"location":"index.html#part-1-build-a-hard-hat-object-detection-model","text":"","title":"Part 1 - Build a Hard Hat Object Detection Model"},{"location":"index.html#section-1-label-a-dataset-of-hard-hat-images-with-white-blue-yellow-person-objects","text":"This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects Instructions : Label Hard Hat Data","title":"Section 1 - Label a dataset of hard hat images with White / Blue / Yellow / Person objects"},{"location":"index.html#section-2-create-ibm-cloud-resources","text":"This section shows you how to create an instance of Cloud Object Storage (COS) in IBM Cloud. Create a COS bucket and upload the hard hat dataset to COS. Finally create Watson Machine Learning instance. Instructions : Create IBM Cloud Resources","title":"Section 2 - Create IBM Cloud resources"},{"location":"index.html#section-3-train-and-test-a-hard-hat-detection-model","text":"This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop. Instructions : Train your Hard Hat model","title":"Section 3 - Train and Test a Hard Hat Detection Model"},{"location":"index.html#part-2-configure-a-jetson-nano","text":"","title":"Part 2 - Configure a Jetson Nano"},{"location":"index.html#section-4-jetson-nano-setup","text":"This section shows you how to configure a Jetson Nano to run object detection. Instructions : Configure Nvidia Jetson Nano","title":"Section 4 - Jetson Nano setup"},{"location":"index.html#section-5-deploy-and-test-hard-hat-object-detection","text":"This section shows you how to download and test the hard hat model. Instructions : Deploy / Test Model on the Jetson","title":"Section 5 - Deploy and Test Hard Hat Object Detection"},{"location":"index.html#part-3-open-horizon","text":"","title":"Part 3 - Open Horizon"},{"location":"index.html#section-6-open-horizon-exchange-hub","text":"This section guides you through the set up of a laptop with HZN Exchange Hub Services Instructions : Open Horizon Exchange Hub Services","title":"Section 6 - Open Horizon Exchange Hub"},{"location":"index.html#section-7-open-horizon-client-setup","text":"Learn how to install Open Horizon HZN anax client and register the Jetson HZN anax client into your hub. Instructions : Open Horizon agent on the Jetson","title":"Section 7 - Open Horizon client setup"},{"location":"index.html#part-4-","text":"","title":"Part 4 -"},{"location":"index.html#section-8-containizer-your-model-in-a-docker-image","text":"This section shows you how to build a docker image with a model Instructions : Build a Docker Image","title":"Section 8 - Containizer your model in a Docker image"},{"location":"index.html#section-9-register-and-deploy-the-pattern-to-the-edge","text":"This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device. Instructions : Deploy Horizon Pattern Enjoy! Give me feedback if you have suggestions on how to improve this workshop.","title":"Section 9 - Register and deploy the Pattern to the edge"},{"location":"index.html#license","text":"This workshop and code examples are licensed under the Apache Software License, Version 2. Separate third party code objects invoked within this code pattern are licensed by their respective providers pursuant to their own separate licenses. Contributions are subject to the Developer Certificate of Origin, Version 1.1 (DCO) and the Apache Software License, Version 2 . Apache Software License (ASL) FAQ","title":"License"},{"location":"part1/index.html","text":"Part 1 - Build a Hard Hat Object Detection Model \u00b6 Section 1 - Label a dataset of hard hat images with White / Blue / Yellow / Person objects \u00b6 This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects Instructions : Label Hard Hat Data Section 2 - Create IBM Cloud resources \u00b6 This section shows you how to create an instance of Cloud Object Storage (COS) in IBM Cloud. Create a COS bucket and upload the hard hat dataset to COS. Finally create Watson Machine Learning instance. Instructions : Create IBM Cloud Resources Section 3 - Train and Test a Hard Hat Detection Model \u00b6 This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop. Instructions : Train your Hard Hat model","title":"Introduction"},{"location":"part1/index.html#part-1-build-a-hard-hat-object-detection-model","text":"","title":"Part 1 - Build a Hard Hat Object Detection Model"},{"location":"part1/index.html#section-1-label-a-dataset-of-hard-hat-images-with-white-blue-yellow-person-objects","text":"This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects Instructions : Label Hard Hat Data","title":"Section 1 - Label a dataset of hard hat images with White / Blue / Yellow / Person objects"},{"location":"part1/index.html#section-2-create-ibm-cloud-resources","text":"This section shows you how to create an instance of Cloud Object Storage (COS) in IBM Cloud. Create a COS bucket and upload the hard hat dataset to COS. Finally create Watson Machine Learning instance. Instructions : Create IBM Cloud Resources","title":"Section 2 - Create IBM Cloud resources"},{"location":"part1/index.html#section-3-train-and-test-a-hard-hat-detection-model","text":"This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop. Instructions : Train your Hard Hat model","title":"Section 3 - Train and Test a Hard Hat Detection Model"},{"location":"part1/CLOUDSETUP.html","text":"Create IBM Cloud resources \u00b6 This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects Lab Objectives \u00b6 In this lab you will learn how to: Create an instance of Cloud Object Storage (COS) in IBM Cloud Upload the hard hat dataset to COS Create Watson Machine Learning instance Login to IBM Cloud \u00b6 Visit IBM Cloud and, if you don't yet have an account, register for a (free) IBM Cloud Lite account. Create a Cloud Object Storage instance \u00b6 IBM Cloud will provision 25GB for free. Visit the IBM Cloud Catalog COS page. Click on the Create button Create a Cloud Object Storage bucket \u00b6 Click on the Create bucket button Select a Standard predefined bucket Name your bucket hardhat-detection-<your-initials> The bucket name must be unique across the whole IBM Cloud Object Storage system Press the Next button Unzip the hardhat-dataset.zip that was downloaded in the previous section. Make certain you extract / unzip the .zip file. Navigate to the extracted files on your computer Drag the _annotations.json into the bucket. A progress bar and a confirmation will display. Hold off on uploading the images folder until the next step where you will be able to upload the entire folder. Press the Next button. On the next panel, scroll down and press the View bucket configuration button. On the next panel, in the left navigation sidebar, click on Objects Then, expand the Upload dropdown, and choose Folders In the popup dialog, click on Select folders A operating system file dialog will open, navigate and select the images folder that you have extracted. A progress bar and a confirmation will display as the files are uploaded. Congratulations - You have uploaded the hardhat model dataset to IBM Cloud Object Storage. Create an Watson Machine Learning Instance \u00b6 Visit the IBM Cloud Catalog Search for Watson Machine Learning Click on the Machine Learning tile Click on the Create button You now have a Watson Machine Learning instance which will be used to train your Hard Hat model. You are now ready to train your Tensorflow hard hat model, so proceed to the next Train Model section .","title":"Create IBM Cloud Resources"},{"location":"part1/CLOUDSETUP.html#create-ibm-cloud-resources","text":"This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects","title":"Create IBM Cloud resources"},{"location":"part1/CLOUDSETUP.html#lab-objectives","text":"In this lab you will learn how to: Create an instance of Cloud Object Storage (COS) in IBM Cloud Upload the hard hat dataset to COS Create Watson Machine Learning instance","title":"Lab Objectives"},{"location":"part1/CLOUDSETUP.html#login-to-ibm-cloud","text":"Visit IBM Cloud and, if you don't yet have an account, register for a (free) IBM Cloud Lite account.","title":"Login to IBM Cloud"},{"location":"part1/CLOUDSETUP.html#create-a-cloud-object-storage-instance","text":"IBM Cloud will provision 25GB for free. Visit the IBM Cloud Catalog COS page. Click on the Create button","title":"Create a Cloud Object Storage instance"},{"location":"part1/CLOUDSETUP.html#create-a-cloud-object-storage-bucket","text":"Click on the Create bucket button Select a Standard predefined bucket Name your bucket hardhat-detection-<your-initials> The bucket name must be unique across the whole IBM Cloud Object Storage system Press the Next button Unzip the hardhat-dataset.zip that was downloaded in the previous section. Make certain you extract / unzip the .zip file. Navigate to the extracted files on your computer Drag the _annotations.json into the bucket. A progress bar and a confirmation will display. Hold off on uploading the images folder until the next step where you will be able to upload the entire folder. Press the Next button. On the next panel, scroll down and press the View bucket configuration button. On the next panel, in the left navigation sidebar, click on Objects Then, expand the Upload dropdown, and choose Folders In the popup dialog, click on Select folders A operating system file dialog will open, navigate and select the images folder that you have extracted. A progress bar and a confirmation will display as the files are uploaded. Congratulations - You have uploaded the hardhat model dataset to IBM Cloud Object Storage.","title":"Create a Cloud Object Storage bucket"},{"location":"part1/CLOUDSETUP.html#create-an-watson-machine-learning-instance","text":"Visit the IBM Cloud Catalog Search for Watson Machine Learning Click on the Machine Learning tile Click on the Create button You now have a Watson Machine Learning instance which will be used to train your Hard Hat model. You are now ready to train your Tensorflow hard hat model, so proceed to the next Train Model section .","title":"Create an Watson Machine Learning Instance"},{"location":"part1/LABEL.html","text":"Label a dataset of hard hat images with White / Blue / Yellow / Person objects \u00b6 This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects. Lab Objectives \u00b6 In this lab you will learn how to: Label a dataset of hard hat images with White / Blue / Yellow / Person objects using Cloud Annotations Hard Hat Image Data Set \u00b6 Create a set of videos with individuals wearing hardhats in several industrial factory settings. Make sure to include varied scenarios with different lighting conditions. If you have different colored hats that you want to recognize, you might record video of people wearing blue, white, yellow hard hats. Use a tool to capture frames at the desired time interval from the videos in your data set. Label each frame with objects that are of interest - blue, white, yellow hard hats and people. Draw bounding boxes around the objects. Repeat this step for all frames. To proceed with this code pattern example, several hundred annotated and labeled hard hat images are provided in this repository. Cloud Annotations \u00b6 Cloud Annotations makes labeling images and training machine learning models easy. Whether you\u2019ve never touched a line of code in your life or you\u2019re a TensorFlow ninja, the Cloud Annotations docs will help you build what you need. The Cloud Annotations tool will help you prepare the training data . To sound the alarms, our worker safety use case needs to identify localized hard hat objects in the frames. A classification model can tell you what an image is and how confident it is about it\u2019s decision. An localized object detection model can provide you with much more information: - Location : The coordinates and area of where the object is in the image. - Count : The number of objects found in the image. - Size : How large the object is with respect to the image dimensions. Cloud Annotations stores the annotated labeled metadata in a file named _annotations.json and images in a directory named images . Cloud Annotations stores this training set in IBM Cloud Object Storage. Download the Hard Hat training dataset \u00b6 Download the Hart Hat annotated labeled metadata and images (50MB zipfile) to your system. In the next section we will upload the training data to IBM Cloud Object Storage (COS). You are now ready to set up your IBM Cloud resources, so proceed to the next Cloud Setup section .","title":"Label Hard Hat Data"},{"location":"part1/LABEL.html#label-a-dataset-of-hard-hat-images-with-white-blue-yellow-person-objects","text":"This section shows you how to use Cloud Annotations to label a dataset of hard hat images with White / Blue / Yellow / Person objects.","title":"Label a dataset of hard hat images with White / Blue / Yellow / Person objects"},{"location":"part1/LABEL.html#lab-objectives","text":"In this lab you will learn how to: Label a dataset of hard hat images with White / Blue / Yellow / Person objects using Cloud Annotations","title":"Lab Objectives"},{"location":"part1/LABEL.html#hard-hat-image-data-set","text":"Create a set of videos with individuals wearing hardhats in several industrial factory settings. Make sure to include varied scenarios with different lighting conditions. If you have different colored hats that you want to recognize, you might record video of people wearing blue, white, yellow hard hats. Use a tool to capture frames at the desired time interval from the videos in your data set. Label each frame with objects that are of interest - blue, white, yellow hard hats and people. Draw bounding boxes around the objects. Repeat this step for all frames. To proceed with this code pattern example, several hundred annotated and labeled hard hat images are provided in this repository.","title":"Hard Hat Image Data Set"},{"location":"part1/LABEL.html#cloud-annotations","text":"Cloud Annotations makes labeling images and training machine learning models easy. Whether you\u2019ve never touched a line of code in your life or you\u2019re a TensorFlow ninja, the Cloud Annotations docs will help you build what you need. The Cloud Annotations tool will help you prepare the training data . To sound the alarms, our worker safety use case needs to identify localized hard hat objects in the frames. A classification model can tell you what an image is and how confident it is about it\u2019s decision. An localized object detection model can provide you with much more information: - Location : The coordinates and area of where the object is in the image. - Count : The number of objects found in the image. - Size : How large the object is with respect to the image dimensions. Cloud Annotations stores the annotated labeled metadata in a file named _annotations.json and images in a directory named images . Cloud Annotations stores this training set in IBM Cloud Object Storage.","title":"Cloud Annotations"},{"location":"part1/LABEL.html#download-the-hard-hat-training-dataset","text":"Download the Hart Hat annotated labeled metadata and images (50MB zipfile) to your system. In the next section we will upload the training data to IBM Cloud Object Storage (COS). You are now ready to set up your IBM Cloud resources, so proceed to the next Cloud Setup section .","title":"Download the Hard Hat training dataset"},{"location":"part1/TRAIN.html","text":"Train and Test a Hard Hat Detection Model \u00b6 This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop. Lab Objectives \u00b6 In this lab you will learn how to: Install Cloud Annotations cacli Train a Tensorflow model Download the model with the cacli git clone object-detection-react Test model on your laptop Cloud Annotations cacli Install \u00b6 Now that the Hard Hat training data set has been uploaded to IBM Cloud Object Storage, we can train our model using the Cloud Annotations command line interface, cacli The cacli is available for Linux, Mac, Windows. Follow the download and installation instructions Cloud Annotations - Login / Train \u00b6 Login to your IBM Cloud account using cacli login and answer the OTP prompts $ cacli login Train your hard hat object detection model $ cacli train Select your hardhat-detection-<your_initials> Bucket. Note the training run Model ID List training runs $ cacli list - Monitor the progress of your training runs $ cacli progress model-58sdpqyx - Wait for the model training to complete (15-30 minutes) - Download the model $ cacli download model-58sdpqyx success download complete Set up a Object Detection web application on your local system \u00b6 The Cloud Annotations github repository includes an object detection react web application that you can install locally. Follow the README.md instructions (or) Run these commands: $ git clone git@github.com:cloud-annotations/object-detection-react.git $ cd object-detection-react $ npm install Copy the model_web directory created by the cacli download and paste it into the public folder of this repo directory. On my Linux system I just create a symlink. Start the server $ npm start Test the Hard Hat model on your laptop \u00b6 Now that the object detection react web application is running, open http://localhost:3000 to view it in the browser. Share your laptop camera with the browser tab. Find your hard hat and observe the hard hat model prediction. Congratulations \u00b6 You are now ready to build a Node-RED Dashboard application that will incorporate this model.","title":"Train your Hard Hat model"},{"location":"part1/TRAIN.html#train-and-test-a-hard-hat-detection-model","text":"This section demonstrates how to install the Cloud Annotations cacli, train a Tensorflow model, download the model, set up a sample localhost web application and test model on your laptop.","title":"Train and Test a Hard Hat Detection Model"},{"location":"part1/TRAIN.html#lab-objectives","text":"In this lab you will learn how to: Install Cloud Annotations cacli Train a Tensorflow model Download the model with the cacli git clone object-detection-react Test model on your laptop","title":"Lab Objectives"},{"location":"part1/TRAIN.html#cloud-annotations-cacli-install","text":"Now that the Hard Hat training data set has been uploaded to IBM Cloud Object Storage, we can train our model using the Cloud Annotations command line interface, cacli The cacli is available for Linux, Mac, Windows. Follow the download and installation instructions","title":"Cloud Annotations cacli Install"},{"location":"part1/TRAIN.html#cloud-annotations-login-train","text":"Login to your IBM Cloud account using cacli login and answer the OTP prompts $ cacli login Train your hard hat object detection model $ cacli train Select your hardhat-detection-<your_initials> Bucket. Note the training run Model ID List training runs $ cacli list - Monitor the progress of your training runs $ cacli progress model-58sdpqyx - Wait for the model training to complete (15-30 minutes) - Download the model $ cacli download model-58sdpqyx success download complete","title":"Cloud Annotations - Login / Train"},{"location":"part1/TRAIN.html#set-up-a-object-detection-web-application-on-your-local-system","text":"The Cloud Annotations github repository includes an object detection react web application that you can install locally. Follow the README.md instructions (or) Run these commands: $ git clone git@github.com:cloud-annotations/object-detection-react.git $ cd object-detection-react $ npm install Copy the model_web directory created by the cacli download and paste it into the public folder of this repo directory. On my Linux system I just create a symlink. Start the server $ npm start","title":"Set up a Object Detection web application on your local system"},{"location":"part1/TRAIN.html#test-the-hard-hat-model-on-your-laptop","text":"Now that the object detection react web application is running, open http://localhost:3000 to view it in the browser. Share your laptop camera with the browser tab. Find your hard hat and observe the hard hat model prediction.","title":"Test the Hard Hat model on your laptop"},{"location":"part1/TRAIN.html#congratulations","text":"You are now ready to build a Node-RED Dashboard application that will incorporate this model.","title":"Congratulations"},{"location":"part2/index.html","text":"Introducing Node-RED \u00b6 Node-RED is a programming tool for building event-driven application. It takes a low-code approach - which means rather than write lots of code, you build applications by using its visual editor to create flows of nodes that describe what should happen when particular events happen. For example, the HTTP In node can be configured to list on a particular path. When an HTTP request arrives on that path the node is triggered. It generates a message containing information about the request and passes it on to the nodes it is wired to. They in turn do whatever work they need to do using the message. Such as generating HTML content and adding it to the message before being passed on through the flow. In this example, the flow ends with an HTTP Response node which responds to the original HTTP request using the information in the message. Next Steps \u00b6 In this part of the tutorial you will: Install Node-RED Install extra nodes into the palette Create your initial Hard Hat Detection Dashboard Add TensorFlow to the Hard Hat Detection dashboard Display detected objects on the dashboard","title":"Introduction"},{"location":"part2/index.html#introducing-node-red","text":"Node-RED is a programming tool for building event-driven application. It takes a low-code approach - which means rather than write lots of code, you build applications by using its visual editor to create flows of nodes that describe what should happen when particular events happen. For example, the HTTP In node can be configured to list on a particular path. When an HTTP request arrives on that path the node is triggered. It generates a message containing information about the request and passes it on to the nodes it is wired to. They in turn do whatever work they need to do using the message. Such as generating HTML content and adding it to the message before being passed on through the flow. In this example, the flow ends with an HTTP Response node which responds to the original HTTP request using the information in the message.","title":"Introducing Node-RED"},{"location":"part2/index.html#next-steps","text":"In this part of the tutorial you will: Install Node-RED Install extra nodes into the palette Create your initial Hard Hat Detection Dashboard Add TensorFlow to the Hard Hat Detection dashboard Display detected objects on the dashboard","title":"Next Steps"},{"location":"part2/adding-tf.html","text":"TensorFlow in Node-RED \u00b6 Installing TensorFlow nodes \u00b6 For this workshop, we're going to use the node-red-contrib-cloud-annotations module that provides the tf model node. This module can be installed from the Manage Palette option in the editor, or by running the following command in ~/.node-red : npm install node-red-contrib-cloud-annotations This will install the module and the TensorFlow library it depends on. Connecting TensorFlow to the WebCam \u00b6 In this part, we'll setup the TensorFlow node to receive images from the WebCam. Add an instance of the tf coco ssd node from the \"analysis\" category of the palette into your workspace. Wire the output of WebCam node to the input of the tf node. Make sure the WebCam node is configured to capture jpeg images - we said we'd remind you about this. Add a Debug node and connect it to the output of the tf node. Click the Deploy button to save your changes. Muting Debug nodes In this screenshot you can see I have muted the Debug node attached to the webcam node by clicking its button in the workspace. This can be useful to turn off different bits of Debug without unwiring or removing the nodes entirely. On the dashboard, make sure your webcam can see you and click the capture button. Switch back to the Node-RED editor and open the Debug sidebar panel. You should see the message sent by the tf node. Its payload consists of a list of the objects it has detected. Each entry in the list has: class - the type of object score - the confidence level of the detection, from 0 to 1 . bbox - an array giving the corners of the bounding box surrounding the detected object msg.payload format Each of the TensorFlow nodes uses a slightly different object format. For example, the node-red-contrib-tf-* nodes set the className property rather than class as we have here. If you experiement with the other nodes make sure you read their documentation and use the Debug node to understand their message format. Next Steps \u00b6 With TensorFlow integrated into the dashboard, the next task is to display the detected objects on the dashboard .","title":"TensorFlow in Node-RED"},{"location":"part2/adding-tf.html#tensorflow-in-node-red","text":"","title":"TensorFlow in Node-RED"},{"location":"part2/adding-tf.html#installing-tensorflow-nodes","text":"For this workshop, we're going to use the node-red-contrib-cloud-annotations module that provides the tf model node. This module can be installed from the Manage Palette option in the editor, or by running the following command in ~/.node-red : npm install node-red-contrib-cloud-annotations This will install the module and the TensorFlow library it depends on.","title":"Installing TensorFlow nodes"},{"location":"part2/adding-tf.html#connecting-tensorflow-to-the-webcam","text":"In this part, we'll setup the TensorFlow node to receive images from the WebCam. Add an instance of the tf coco ssd node from the \"analysis\" category of the palette into your workspace. Wire the output of WebCam node to the input of the tf node. Make sure the WebCam node is configured to capture jpeg images - we said we'd remind you about this. Add a Debug node and connect it to the output of the tf node. Click the Deploy button to save your changes. Muting Debug nodes In this screenshot you can see I have muted the Debug node attached to the webcam node by clicking its button in the workspace. This can be useful to turn off different bits of Debug without unwiring or removing the nodes entirely. On the dashboard, make sure your webcam can see you and click the capture button. Switch back to the Node-RED editor and open the Debug sidebar panel. You should see the message sent by the tf node. Its payload consists of a list of the objects it has detected. Each entry in the list has: class - the type of object score - the confidence level of the detection, from 0 to 1 . bbox - an array giving the corners of the bounding box surrounding the detected object msg.payload format Each of the TensorFlow nodes uses a slightly different object format. For example, the node-red-contrib-tf-* nodes set the className property rather than class as we have here. If you experiement with the other nodes make sure you read their documentation and use the Debug node to understand their message format.","title":"Connecting TensorFlow to the WebCam"},{"location":"part2/adding-tf.html#next-steps","text":"With TensorFlow integrated into the dashboard, the next task is to display the detected objects on the dashboard .","title":"Next Steps"},{"location":"part2/create-dashboard.html","text":"Create a dashboard \u00b6 In this part of the workshop you will begin to create your Hard Hat Detection application. Dashboard Layouts \u00b6 The Dashboard uses a grid based layout. Widgets, such as buttons or text boxes, are given a size in grid-cells. They are packed into a group that has a defined width. The Groups are then placed on a Tab - laid out using a flow-based layout, filling the width of the page before wrapping. This arrangement provides some flexibility in how the page displays on different screen sizes. Dashboard Sidebar \u00b6 Within the editor, the Dashboard module provides a sidebar where you can customize its features as well as manage its tabs, groups and widgets. Open the Dashboard sidebar Click the + tab button to create a new tab. Hover over the newly added tab and click the edit button. In the edit dialog, give the tab a name of AI Photo Booth and click Update to close the dialog. Hover over the tab again and click the + group button. Edit the new group and set its properties: Set the name to 'WebCam' Set the width to 10 by clicking the button and dragging the box out to 10 units wide. Untick the 'Display group name' option This has created the initial layout components needed for the dashboard. You can now start to add content. To access the Dashboard, click the button in the top-right corner of the Dashboard sidebar. This will open the Dashboard in a new browser tab. Next Steps \u00b6 The next task is to add some different controls to the dashboard .","title":"Create a dashboard"},{"location":"part2/create-dashboard.html#create-a-dashboard","text":"In this part of the workshop you will begin to create your Hard Hat Detection application.","title":"Create a dashboard"},{"location":"part2/create-dashboard.html#dashboard-layouts","text":"The Dashboard uses a grid based layout. Widgets, such as buttons or text boxes, are given a size in grid-cells. They are packed into a group that has a defined width. The Groups are then placed on a Tab - laid out using a flow-based layout, filling the width of the page before wrapping. This arrangement provides some flexibility in how the page displays on different screen sizes.","title":"Dashboard Layouts"},{"location":"part2/create-dashboard.html#dashboard-sidebar","text":"Within the editor, the Dashboard module provides a sidebar where you can customize its features as well as manage its tabs, groups and widgets. Open the Dashboard sidebar Click the + tab button to create a new tab. Hover over the newly added tab and click the edit button. In the edit dialog, give the tab a name of AI Photo Booth and click Update to close the dialog. Hover over the tab again and click the + group button. Edit the new group and set its properties: Set the name to 'WebCam' Set the width to 10 by clicking the button and dragging the box out to 10 units wide. Untick the 'Display group name' option This has created the initial layout components needed for the dashboard. You can now start to add content. To access the Dashboard, click the button in the top-right corner of the Dashboard sidebar. This will open the Dashboard in a new browser tab.","title":"Dashboard Sidebar"},{"location":"part2/create-dashboard.html#next-steps","text":"The next task is to add some different controls to the dashboard .","title":"Next Steps"},{"location":"part2/display-objects.html","text":"Displaying the detected objects \u00b6 In this part we're going to display the detected objects on the dashboard in two different ways. First we will display an annotated version of the captured image with all of the objects highlighted. We will then add a table to the dashboard that lists them out. Displaying an annotated image \u00b6 The tf coco ssd node has an option to output an annotated version of the image with all of the detected objects highlighted. The image is set on the msg.image message property. Edit the tf node and configure the \"Passthru\" field to Annotated Image Add a Change node, wired to the output of the tf node and configure it to move msg.image to msg.payload . Wire the Change node to the input of the WebCam node. Click the Deploy button to save your changes. Laying out flows With this latest addition, you can see we now have wires crossing each other and looping back on themselves. As flows evolve, their wiring can become quite complex. It is always worth spending some time trying to find a layout that remains 'readable'. There is a Flow Developer guide in the Node-RED documenation that provides a number of tips on how to layout flows. Now when you take an image on the dashboard, you should see the annotated version of the image. Adding a table of objects \u00b6 Install the module node-red-node-ui-table using the Manage Palette option in the editor, or be running the following command in ~/.node-red : npm install node-red-node-ui-table This adds the ui_table node to the palette which can be used to display tabular data. In the Dashboard sidebar of the Node-RED editor, hover over the AI Photo Booth tab and click the + group button. Edit the new group and set its properties: Set the name to 'Objects' Set the width to 6 by clicking the button and dragging the box out to 6 units wide. Untick the 'Display group name' option Add a new ui_table node from the \"dashboard\" section of the palette into your workspace. Edit its properties as follows: Add it to the 'Objects' group Set its size to 6x8 Add two columns by clicking the + add button at the bottom. Configure them as: Property: class , Title: Object Type Property: score , Title: Score , Format: Progress (0-100) Add a Change node to the workspace. Configure it to set msg.payload to the expression $append([],payload.{\"class\":class,\"score\":score*100,\"bbox\":bbox}) Note Make sure you select the expression type for the to field of the Change node. This uses the JSONata expression language. Create the following wires between nodes: wire the output of the tf node to the Change node. wire the output of the Change node to the Table node Click the Deploy button to save your changes. Now when you capture an image on the dashboard, the table should list the detected objects. Side Quest - Star Ratings The JSONata expression used in the Change node mapped the score property of each detected object from its original 0-1 range to the 0-100 range expected by the ui_table node's \"Progress\" column type. The table supports a number of other formats of displaying numeric values. For example, it can map a number in the 0-100 range to a traffic light colour. It can also display a value in the range 0-5 as a number of stars. Edit the table node to display the score using the star format. See if you can modify the expression in the Change node to map the original score to the required 0-5 range. Side Quest - Clear the table With the current dashboard, when an image is captured it gets displayed in place of the live web cam view until the clear button is clicked. However clicking the button does not clear the table we've just added. Using what you've learnt so far, build a flow between the Clear button and the table node that will clear the table when the button is clicked. Hint: think about what payload must be passed to the table in order to clear it. Next Steps \u00b6 Now that we have an application, let's set up and run it on a Nvidia Jetson Nano. Proceed to the next Jetson Setup section .","title":"Displaying detected objects"},{"location":"part2/display-objects.html#displaying-the-detected-objects","text":"In this part we're going to display the detected objects on the dashboard in two different ways. First we will display an annotated version of the captured image with all of the objects highlighted. We will then add a table to the dashboard that lists them out.","title":"Displaying the detected objects"},{"location":"part2/display-objects.html#displaying-an-annotated-image","text":"The tf coco ssd node has an option to output an annotated version of the image with all of the detected objects highlighted. The image is set on the msg.image message property. Edit the tf node and configure the \"Passthru\" field to Annotated Image Add a Change node, wired to the output of the tf node and configure it to move msg.image to msg.payload . Wire the Change node to the input of the WebCam node. Click the Deploy button to save your changes. Laying out flows With this latest addition, you can see we now have wires crossing each other and looping back on themselves. As flows evolve, their wiring can become quite complex. It is always worth spending some time trying to find a layout that remains 'readable'. There is a Flow Developer guide in the Node-RED documenation that provides a number of tips on how to layout flows. Now when you take an image on the dashboard, you should see the annotated version of the image.","title":"Displaying an annotated image"},{"location":"part2/display-objects.html#adding-a-table-of-objects","text":"Install the module node-red-node-ui-table using the Manage Palette option in the editor, or be running the following command in ~/.node-red : npm install node-red-node-ui-table This adds the ui_table node to the palette which can be used to display tabular data. In the Dashboard sidebar of the Node-RED editor, hover over the AI Photo Booth tab and click the + group button. Edit the new group and set its properties: Set the name to 'Objects' Set the width to 6 by clicking the button and dragging the box out to 6 units wide. Untick the 'Display group name' option Add a new ui_table node from the \"dashboard\" section of the palette into your workspace. Edit its properties as follows: Add it to the 'Objects' group Set its size to 6x8 Add two columns by clicking the + add button at the bottom. Configure them as: Property: class , Title: Object Type Property: score , Title: Score , Format: Progress (0-100) Add a Change node to the workspace. Configure it to set msg.payload to the expression $append([],payload.{\"class\":class,\"score\":score*100,\"bbox\":bbox}) Note Make sure you select the expression type for the to field of the Change node. This uses the JSONata expression language. Create the following wires between nodes: wire the output of the tf node to the Change node. wire the output of the Change node to the Table node Click the Deploy button to save your changes. Now when you capture an image on the dashboard, the table should list the detected objects. Side Quest - Star Ratings The JSONata expression used in the Change node mapped the score property of each detected object from its original 0-1 range to the 0-100 range expected by the ui_table node's \"Progress\" column type. The table supports a number of other formats of displaying numeric values. For example, it can map a number in the 0-100 range to a traffic light colour. It can also display a value in the range 0-5 as a number of stars. Edit the table node to display the score using the star format. See if you can modify the expression in the Change node to map the original score to the required 0-5 range. Side Quest - Clear the table With the current dashboard, when an image is captured it gets displayed in place of the live web cam view until the clear button is clicked. However clicking the button does not clear the table we've just added. Using what you've learnt so far, build a flow between the Clear button and the table node that will clear the table when the button is clicked. Hint: think about what payload must be passed to the table in order to clear it.","title":"Adding a table of objects"},{"location":"part2/display-objects.html#next-steps","text":"Now that we have an application, let's set up and run it on a Nvidia Jetson Nano. Proceed to the next Jetson Setup section .","title":"Next Steps"},{"location":"part2/install.html","text":"Installing Node-RED \u00b6 Node-RED is published as a node.js module available on npm, as well as a container available on Docker Hub. The full guide for installing and running Node-RED is available here . Linux The following steps assume you are running on Windows or OSX. If you are running on a Linux OS, or a device like a Raspberry Pi, the project provides a set of install scripts that will get node, npm and Node-RED all installed at the latest stable versions. Refer to the docs linked above. You must have a supported version of Node.js installed. Node-RED supports the Active and LTS releases, 12.x and 14.x. You can then install Node-RED as a global module with the command: npm install -g --unsafe-perm node-red Depending on your Node.js installation, you may need to run this command using sudo . The install log output may contain some warnings - these can be ignored as long as the output ends with something like: + node-red@1.2.2 added 332 packages from 341 contributors in 18.494s Running Node-RED \u00b6 Once installed, you should now have the node-red command available to run. Command not found If you do not have the node-red command available it may be a problem with your PATH configuration. Find where your global node modules are installed by running: npm get prefix Then ensure the bin subdirectory of that location is on your PATH . When you run node-red , the log output will appear 23 Oct 00:12:01 - [info] Welcome to Node-RED =================== 23 Oct 00:12:01 - [info] Node-RED version: v1.2.2 23 Oct 00:12:01 - [info] Node.js version: v12.19.0 23 Oct 00:12:01 - [info] Darwin 18.7.0 x64 LE 23 Oct 00:12:01 - [info] Loading palette nodes 23 Oct 00:12:03 - [info] Settings file : /Users/nol/.node-red/settings.js 23 Oct 00:12:03 - [info] User directory : /Users/nol/.node-red 23 Oct 00:12:03 - [info] Server now running at http://127.0.0.1:1880/ 23 Oct 00:12:03 - [info] Flows file : /Users/nol/.node-red/flows.json 23 Oct 00:12:03 - [info] Starting flows 23 Oct 00:12:03 - [info] Started flows This output contains an important piece of information you will need - the location of your User directory . Accessing the Node-RED editor \u00b6 Assuming you are running Node-RED on your local computer, open a browser and access the url http://127.0.0.1:1880/ . This will load the Node-RED editor - the tool used to build your applications. Next Steps \u00b6 The next task is to Install extra nodes into the palette .","title":"Install Node-RED"},{"location":"part2/install.html#installing-node-red","text":"Node-RED is published as a node.js module available on npm, as well as a container available on Docker Hub. The full guide for installing and running Node-RED is available here . Linux The following steps assume you are running on Windows or OSX. If you are running on a Linux OS, or a device like a Raspberry Pi, the project provides a set of install scripts that will get node, npm and Node-RED all installed at the latest stable versions. Refer to the docs linked above. You must have a supported version of Node.js installed. Node-RED supports the Active and LTS releases, 12.x and 14.x. You can then install Node-RED as a global module with the command: npm install -g --unsafe-perm node-red Depending on your Node.js installation, you may need to run this command using sudo . The install log output may contain some warnings - these can be ignored as long as the output ends with something like: + node-red@1.2.2 added 332 packages from 341 contributors in 18.494s","title":"Installing Node-RED"},{"location":"part2/install.html#running-node-red","text":"Once installed, you should now have the node-red command available to run. Command not found If you do not have the node-red command available it may be a problem with your PATH configuration. Find where your global node modules are installed by running: npm get prefix Then ensure the bin subdirectory of that location is on your PATH . When you run node-red , the log output will appear 23 Oct 00:12:01 - [info] Welcome to Node-RED =================== 23 Oct 00:12:01 - [info] Node-RED version: v1.2.2 23 Oct 00:12:01 - [info] Node.js version: v12.19.0 23 Oct 00:12:01 - [info] Darwin 18.7.0 x64 LE 23 Oct 00:12:01 - [info] Loading palette nodes 23 Oct 00:12:03 - [info] Settings file : /Users/nol/.node-red/settings.js 23 Oct 00:12:03 - [info] User directory : /Users/nol/.node-red 23 Oct 00:12:03 - [info] Server now running at http://127.0.0.1:1880/ 23 Oct 00:12:03 - [info] Flows file : /Users/nol/.node-red/flows.json 23 Oct 00:12:03 - [info] Starting flows 23 Oct 00:12:03 - [info] Started flows This output contains an important piece of information you will need - the location of your User directory .","title":"Running Node-RED"},{"location":"part2/install.html#accessing-the-node-red-editor","text":"Assuming you are running Node-RED on your local computer, open a browser and access the url http://127.0.0.1:1880/ . This will load the Node-RED editor - the tool used to build your applications.","title":"Accessing the Node-RED editor"},{"location":"part2/install.html#next-steps","text":"The next task is to Install extra nodes into the palette .","title":"Next Steps"},{"location":"part2/installing-nodes.html","text":"Installing Nodes \u00b6 The building blocks of any Node-RED application are the nodes in its palette. Node-RED comes with a number of core nodes that provide the basic components, but the palette can be easily extended by installing additional nodes. Nodes are published as npm modules and the project provides an online catalogue of them at https://flows.nodered.org . There are two ways to install nodes - via the command-line or from within the Node-RED editor. Node-RED Palette Manager \u00b6 To install a node from within the editor, select the Manage Palette option from the main menu. This opens the Palette Manager which shows two tabs - a list of the modules you have installed and a searchable catalogue of modules available to install. Switch to the Install tab and search for random - you should see node-red-node-random in the list below. Click the install button next to it. After a short time the node will be installed and added to the palette. Command-line \u00b6 To install on the command-line, switch to the Node-RED user directory and run the appropriate npm install command. For example: npm install node-red-node-random Node-RED User Directory By default, Node-RED creates a directory called .node-red in the user's home directory. As it starts with a . it may be hidden from view by your file browser. As mentioned in the Install section, Node-RED logs the full path to the user directory when it starts up. If in doubt, check what it says. Note Some nodes will have external dependencies that cannot be automatically installed by Node-RED or npm. You should always check a module's readme for further information. This will be particularly true of some of the TensorFlow nodes we'll be using later in this workshop. Next Steps \u00b6 The next task is to create you first flow in Node-RED .","title":"Installing Nodes"},{"location":"part2/installing-nodes.html#installing-nodes","text":"The building blocks of any Node-RED application are the nodes in its palette. Node-RED comes with a number of core nodes that provide the basic components, but the palette can be easily extended by installing additional nodes. Nodes are published as npm modules and the project provides an online catalogue of them at https://flows.nodered.org . There are two ways to install nodes - via the command-line or from within the Node-RED editor.","title":"Installing Nodes"},{"location":"part2/installing-nodes.html#node-red-palette-manager","text":"To install a node from within the editor, select the Manage Palette option from the main menu. This opens the Palette Manager which shows two tabs - a list of the modules you have installed and a searchable catalogue of modules available to install. Switch to the Install tab and search for random - you should see node-red-node-random in the list below. Click the install button next to it. After a short time the node will be installed and added to the palette.","title":"Node-RED Palette Manager"},{"location":"part2/installing-nodes.html#command-line","text":"To install on the command-line, switch to the Node-RED user directory and run the appropriate npm install command. For example: npm install node-red-node-random Node-RED User Directory By default, Node-RED creates a directory called .node-red in the user's home directory. As it starts with a . it may be hidden from view by your file browser. As mentioned in the Install section, Node-RED logs the full path to the user directory when it starts up. If in doubt, check what it says. Note Some nodes will have external dependencies that cannot be automatically installed by Node-RED or npm. You should always check a module's readme for further information. This will be particularly true of some of the TensorFlow nodes we'll be using later in this workshop.","title":"Command-line"},{"location":"part2/installing-nodes.html#next-steps","text":"The next task is to create you first flow in Node-RED .","title":"Next Steps"},{"location":"part3/index.html","text":"Edge Computing Worker Safety Hard Hat Detection - Jetson Nano setup \u00b6 This section shows you how to configure a Jetson Nano to run object detection. Next Steps \u00b6 In this part of the workshop you will: Set up a Jetson Nano with Ubuntu 5V barrel power supply to provide sufficient power Download and Run the Docker container Test model on the Jetson","title":"Introduction"},{"location":"part3/index.html#edge-computing-worker-safety-hard-hat-detection-jetson-nano-setup","text":"This section shows you how to configure a Jetson Nano to run object detection.","title":"Edge Computing Worker Safety Hard Hat Detection  - Jetson Nano setup"},{"location":"part3/index.html#next-steps","text":"In this part of the workshop you will: Set up a Jetson Nano with Ubuntu 5V barrel power supply to provide sufficient power Download and Run the Docker container Test model on the Jetson","title":"Next Steps"},{"location":"part3/EDGEINFER.html","text":"Train and Test a Hard Hat Detection Model \u00b6 This section shows you how to download and test the hard hat model on your Jetson Nano. Lab Objectives \u00b6 In this lab you will learn how to: Download the hard hat model with the cacli git clone object-detection-react Test model on the Jetson Inferencing on the Jetson \u00b6 In a previous section you uploaded hard hat image training data and trained a object detection model using the Cloud Annotations command line interface, cacli, Tensorflow and Watson Machine Learning. You also downloaded the model and tested the model on your laptop using a browser and your laptop web camera. This section will repeat some of those steps on the Jetson Nano. Cloud Annotations cacli Install \u00b6 Our model is already trained so we can use the Cloud Annotations command line interface, cacli to download it. The cacli is available for Linux, Mac, Windows. Follow the download and installation instructions Cloud Annotations - Login / List Models \u00b6 Login to your IBM Cloud account using cacli login and answer the OTP prompts $ cacli login List training runs and trained models $ cacli list Download your hard hat object detection model \u00b6 Download the model $ cacli download model-58sdpqyx success download complete Install node.js \u00b6 Before we can run the Object Detection web application on your Jetson Nano, these commands install Node.js bash $ curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - $ sudo apt-get install -y nodejs Set up a Object Detection web application on your Jetson Nano \u00b6 The Cloud Annotations github repository includes an object detection react web application that you can install locally. Follow the README.md instructions (or) Run these commands: $ git clone git@github.com:cloud-annotations/object-detection-react.git $ cd object-detection-react $ npm install Move the model_web directory created by the cacli download into the public folder of this repo directory. On the Ubuntu Linux system I just create a symlink. Start the server $ npm start Test the Hard Hat model on your Jetson Nano \u00b6 Now that the object detection react web application is running, launch Chromium on your Jetson Nano and open http://localhost:3000 to view it in the browser. Share your Jetson web camera with the browser tab. Find your hard hat and observe the hard hat model prediction running on the Jetson! Congratulations - Object Detection on the Edge! \u00b6 You are now ready to set up Open Horizon Exchange Hub Services, so proceed to the next Horizon Hub Setup section .","title":"Deploy / Test Model on the Jetson"},{"location":"part3/EDGEINFER.html#train-and-test-a-hard-hat-detection-model","text":"This section shows you how to download and test the hard hat model on your Jetson Nano.","title":"Train and Test a Hard Hat Detection Model"},{"location":"part3/EDGEINFER.html#lab-objectives","text":"In this lab you will learn how to: Download the hard hat model with the cacli git clone object-detection-react Test model on the Jetson","title":"Lab Objectives"},{"location":"part3/EDGEINFER.html#inferencing-on-the-jetson","text":"In a previous section you uploaded hard hat image training data and trained a object detection model using the Cloud Annotations command line interface, cacli, Tensorflow and Watson Machine Learning. You also downloaded the model and tested the model on your laptop using a browser and your laptop web camera. This section will repeat some of those steps on the Jetson Nano.","title":"Inferencing on the Jetson"},{"location":"part3/EDGEINFER.html#cloud-annotations-cacli-install","text":"Our model is already trained so we can use the Cloud Annotations command line interface, cacli to download it. The cacli is available for Linux, Mac, Windows. Follow the download and installation instructions","title":"Cloud Annotations cacli Install"},{"location":"part3/EDGEINFER.html#cloud-annotations-login-list-models","text":"Login to your IBM Cloud account using cacli login and answer the OTP prompts $ cacli login List training runs and trained models $ cacli list","title":"Cloud Annotations - Login / List Models"},{"location":"part3/EDGEINFER.html#download-your-hard-hat-object-detection-model","text":"Download the model $ cacli download model-58sdpqyx success download complete","title":"Download your hard hat object detection model"},{"location":"part3/EDGEINFER.html#install-nodejs","text":"Before we can run the Object Detection web application on your Jetson Nano, these commands install Node.js bash $ curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - $ sudo apt-get install -y nodejs","title":"Install node.js"},{"location":"part3/EDGEINFER.html#set-up-a-object-detection-web-application-on-your-jetson-nano","text":"The Cloud Annotations github repository includes an object detection react web application that you can install locally. Follow the README.md instructions (or) Run these commands: $ git clone git@github.com:cloud-annotations/object-detection-react.git $ cd object-detection-react $ npm install Move the model_web directory created by the cacli download into the public folder of this repo directory. On the Ubuntu Linux system I just create a symlink. Start the server $ npm start","title":"Set up a Object Detection web application on your Jetson Nano"},{"location":"part3/EDGEINFER.html#test-the-hard-hat-model-on-your-jetson-nano","text":"Now that the object detection react web application is running, launch Chromium on your Jetson Nano and open http://localhost:3000 to view it in the browser. Share your Jetson web camera with the browser tab. Find your hard hat and observe the hard hat model prediction running on the Jetson!","title":"Test the Hard Hat model on your Jetson Nano"},{"location":"part3/EDGEINFER.html#congratulations-object-detection-on-the-edge","text":"You are now ready to set up Open Horizon Exchange Hub Services, so proceed to the next Horizon Hub Setup section .","title":"Congratulations - Object Detection on the Edge!"},{"location":"part3/JETSON.html","text":"Jetson Nano setup \u00b6 This section shows you how to configure a Jetson Nano to run object detection. Lab Objectives \u00b6 In this lab you will learn how to: Set up a Jetson Nano with Ubuntu Attach a USB webcam, use a 5V barrel power supply to provide sufficient power Set up a Jetson Nano Developer Kit \u00b6 Nvidia provides thorough setup documentation on the Jetson Nano Developer Kit so this guide will not attempt to reproduce those steps. In addition, there are numerous Internet resources that walk through the configuration and setup. This tutorial will emphasize several parts of those guides that can assure a successful implementation of the Worker Safety and Hard Hat Detection model. 5V Barrel Jack Power Adapter The Jetson Nano can be powered by a Micro-USB 5V 2A power supply but the camera and GPU require additional power to operate. Avoid the frustration of indeterminate results and switch to a 5V barrel jack power supply (4A). Closing the J48 jumper with a standard 2.54mm pitch jumper will switch the power from the micro USB jack to the barrel jack. Follow the recommended instructions to close jumper J48 on the Nano PCB. Once you are powering the Jetson Nano with a 5V barrel jack power supply, you can enable the 'maximum performance model' and the system and inferencing should run faster. sudo nvpmodel -m 0 Attach a Android IP Web camera app \u00b6 Object Detection does not require the highest quality camera. Most images will get scaled down. Internet Connectivity \u00b6 You will need an ethernet cable (the Jetson Nano developer kit does not include WiFi) or a WiFi USB dongle. Now the fun begins! \u00b6 You are now ready to run the hard hat model on the Jetson edge device, so proceed to the next Edge Inferencing section .","title":"Configure Nvidia Jetson Nano"},{"location":"part3/JETSON.html#jetson-nano-setup","text":"This section shows you how to configure a Jetson Nano to run object detection.","title":"Jetson Nano setup"},{"location":"part3/JETSON.html#lab-objectives","text":"In this lab you will learn how to: Set up a Jetson Nano with Ubuntu Attach a USB webcam, use a 5V barrel power supply to provide sufficient power","title":"Lab Objectives"},{"location":"part3/JETSON.html#set-up-a-jetson-nano-developer-kit","text":"Nvidia provides thorough setup documentation on the Jetson Nano Developer Kit so this guide will not attempt to reproduce those steps. In addition, there are numerous Internet resources that walk through the configuration and setup. This tutorial will emphasize several parts of those guides that can assure a successful implementation of the Worker Safety and Hard Hat Detection model.","title":"Set up a Jetson Nano Developer Kit"},{"location":"part3/JETSON.html#attach-a-android-ip-web-camera-app","text":"Object Detection does not require the highest quality camera. Most images will get scaled down.","title":"Attach a Android IP Web camera app"},{"location":"part3/JETSON.html#internet-connectivity","text":"You will need an ethernet cable (the Jetson Nano developer kit does not include WiFi) or a WiFi USB dongle.","title":"Internet Connectivity"},{"location":"part3/JETSON.html#now-the-fun-begins","text":"You are now ready to run the hard hat model on the Jetson edge device, so proceed to the next Edge Inferencing section .","title":"Now the fun begins!"},{"location":"part4/index.html","text":"","title":"Introduction"},{"location":"part4/HZNCLIENT.html","text":"Deploy and Test Hard Hat Object Detection \u00b6 Learn how to install Open Horizon HZN anax client and register the Jetson HZN anax client into your hub. Lab Objectives \u00b6 In this lab you will learn how to: On your Jetson Nano, Install docker Install Open Horizon HZN anax client Register Jetson HZN anax client into Hub \u00b6 You are now ready to containerize the hard hat model in a Docker image, so proceed to the next Docker Model section .","title":"Open Horizon agent on the Jetson"},{"location":"part4/HZNCLIENT.html#deploy-and-test-hard-hat-object-detection","text":"Learn how to install Open Horizon HZN anax client and register the Jetson HZN anax client into your hub.","title":"Deploy and Test Hard Hat Object Detection"},{"location":"part4/HZNCLIENT.html#lab-objectives","text":"In this lab you will learn how to: On your Jetson Nano, Install docker Install Open Horizon HZN anax client Register Jetson HZN anax client into Hub","title":"Lab Objectives"},{"location":"part4/HZNCLIENT.html#_1","text":"You are now ready to containerize the hard hat model in a Docker image, so proceed to the next Docker Model section .","title":""},{"location":"part4/HZNHUB.html","text":"Open Horizon Exchange Hub Set Up \u00b6 This section guides you through the set up of a laptop with HZN Exchange Hub Services Lab Objectives \u00b6 In this lab you will learn how to: Set up a laptop with Horizon Exchange Hub Services \u00b6 You are now ready to set up your Open Horizon anax client on the Jetson Nano, so proceed to the next Horizon Client section .","title":"Open Horizon Exchange Hub Services"},{"location":"part4/HZNHUB.html#open-horizon-exchange-hub-set-up","text":"This section guides you through the set up of a laptop with HZN Exchange Hub Services","title":"Open Horizon Exchange Hub Set Up"},{"location":"part4/HZNHUB.html#lab-objectives","text":"In this lab you will learn how to: Set up a laptop with Horizon Exchange Hub Services","title":"Lab Objectives"},{"location":"part4/HZNHUB.html#_1","text":"You are now ready to set up your Open Horizon anax client on the Jetson Nano, so proceed to the next Horizon Client section .","title":""},{"location":"part5/index.html","text":"Containerization \u00b6 Why create a container? \u00b6 Up to this point in the workshop, you've been developing an application using Node-RED on your local device. The fact the editor and runtime are bundled together makes it very convenient to quickly start building applications. But that model is less suitable when you think about creating applications that run in production, or that need to be distributed to remote devices. You don't want to be using the editor to edit the application directly - you want to be able to develop and test your application locally and then have a controlled way to deploy it into your production environment. In this part of the workshop, we're going to step through the process of wrapping the application as a Docker container. Once the container has been created, it can be deployed just as you would any other container - pushing it to a cloud environment or down to edge devices. Next Steps \u00b6 At the start of this workshop, you enabled the Projects feature. That gave you a git repository you can use to manage your application. In this section we are going to make some updates to the project files to help create a deployable container. This involves: Updating the project's package.json file Adding a settings.js file Adding a Dockerfile","title":"Introduction"},{"location":"part5/index.html#containerization","text":"","title":"Containerization"},{"location":"part5/index.html#why-create-a-container","text":"Up to this point in the workshop, you've been developing an application using Node-RED on your local device. The fact the editor and runtime are bundled together makes it very convenient to quickly start building applications. But that model is less suitable when you think about creating applications that run in production, or that need to be distributed to remote devices. You don't want to be using the editor to edit the application directly - you want to be able to develop and test your application locally and then have a controlled way to deploy it into your production environment. In this part of the workshop, we're going to step through the process of wrapping the application as a Docker container. Once the container has been created, it can be deployed just as you would any other container - pushing it to a cloud environment or down to edge devices.","title":"Why create a container?"},{"location":"part5/index.html#next-steps","text":"At the start of this workshop, you enabled the Projects feature. That gave you a git repository you can use to manage your application. In this section we are going to make some updates to the project files to help create a deployable container. This involves: Updating the project's package.json file Adding a settings.js file Adding a Dockerfile","title":"Next Steps"},{"location":"part5/dockerdeploy.html","text":"Push to Docker Hub \u00b6 The final task is to push the Docker container to Docker Hub so that the IEAM / Horizon Hub Exchange can be configured and so that Edge devices can pull it down.","title":"Deploy container to DockerHub"},{"location":"part5/dockerdeploy.html#push-to-docker-hub","text":"The final task is to push the Docker container to Docker Hub so that the IEAM / Horizon Hub Exchange can be configured and so that Edge devices can pull it down.","title":"Push to Docker Hub"},{"location":"part5/dockerfile.html","text":"Add a Dockerfile \u00b6 The final task is to add a Dockerfile and to build the container. Create the file ~/.node-red/projects/<name-of-project>/Dockerfile with the following contents: FROM node:lts as build RUN apt-get update \\ && apt-get install -y build-essential WORKDIR /data COPY ./package.json /data/ RUN npm install COPY ./settings.js /data/ COPY ./flows.json /data/ COPY ./flows_cred.json /data/ ## Release image FROM node:lts-slim RUN apt-get update RUN mkdir -p /data COPY --from=build /data /data WORKDIR /data ENV PORT 1880 ENV NODE_ENV=production ENV NODE_PATH=/data/node_modules EXPOSE 1880 CMD [\"npm\", \"start\"] This Dockerfile has two parts. It first creates a build image using the latest node:lts image. It installs the build tools, copies in the project's package.json ,installs all of the node modules and then copies in the remaining project files. After that, it creates the real image using the node:lts-slim image - a smaller base image. It copies over the required parts from the build image, sets up some default environment variables and then runs Node-RED. Building the image \u00b6 To build the image, run the following command from the ~/.node-red/projects/<name-of-project>/ directory: docker build . -t node-red-photobooth This will take a few minutes the first time you run it as it will have to download the base images. Subsequent runs will be quicker as those downloads are cached. Running the image locally \u00b6 Once built, you can test the image locally by running: docker run -p 9000:1880 --name photobooth node-red-photobooth Once that runs, you will be able to open http://localhost:9000 to access the photo booth dashboard. Cleaning up Docker To stop the running image, you can run the command: docker stop photobooth To delete the container, run: docker rm photobooth To delete the image, run: docker rmi node-red-photobooth","title":"Build a multistage Dockerfile"},{"location":"part5/dockerfile.html#add-a-dockerfile","text":"The final task is to add a Dockerfile and to build the container. Create the file ~/.node-red/projects/<name-of-project>/Dockerfile with the following contents: FROM node:lts as build RUN apt-get update \\ && apt-get install -y build-essential WORKDIR /data COPY ./package.json /data/ RUN npm install COPY ./settings.js /data/ COPY ./flows.json /data/ COPY ./flows_cred.json /data/ ## Release image FROM node:lts-slim RUN apt-get update RUN mkdir -p /data COPY --from=build /data /data WORKDIR /data ENV PORT 1880 ENV NODE_ENV=production ENV NODE_PATH=/data/node_modules EXPOSE 1880 CMD [\"npm\", \"start\"] This Dockerfile has two parts. It first creates a build image using the latest node:lts image. It installs the build tools, copies in the project's package.json ,installs all of the node modules and then copies in the remaining project files. After that, it creates the real image using the node:lts-slim image - a smaller base image. It copies over the required parts from the build image, sets up some default environment variables and then runs Node-RED.","title":"Add a Dockerfile"},{"location":"part5/dockerfile.html#building-the-image","text":"To build the image, run the following command from the ~/.node-red/projects/<name-of-project>/ directory: docker build . -t node-red-photobooth This will take a few minutes the first time you run it as it will have to download the base images. Subsequent runs will be quicker as those downloads are cached.","title":"Building the image"},{"location":"part5/dockerfile.html#running-the-image-locally","text":"Once built, you can test the image locally by running: docker run -p 9000:1880 --name photobooth node-red-photobooth Once that runs, you will be able to open http://localhost:9000 to access the photo booth dashboard. Cleaning up Docker To stop the running image, you can run the command: docker stop photobooth To delete the container, run: docker rm photobooth To delete the image, run: docker rmi node-red-photobooth","title":"Running the image locally"},{"location":"part5/package.html","text":"Update package.json \u00b6 Adding dependencies \u00b6 Through the workshop you've installed a number of additional modules into Node-RED. We need to make sure they are listed in the project's package.json file so they will get installed into the container. Open the Project Settings dialog from the main menu in the editor ( Projects -> Project Settings ) Switch to the Dependencies tab. You will see a list of the additional modules that are used by the project . Click the 'add to project' next to each one and then close the dialog. That will have updated the package.json file for you. However, for this scenario there's one more dependency we need to add manually - node-red itself. The package.json file can be found in ~/.node-red/projects/<name-of-project>/package.json . Open that file in a text editor and make the following changes: add node-red to the dependencies section - this will ensure Node-RED gets installed when the container is built. \"dependencies\": { \"node-red\": \"1.x\", ... }, Add a scripts section to define a start command - this is how the container will run Node-RED: \"scripts\": { \"start\": \"node --max-old-space-size=256 ./node_modules/node-red/red.js --userDir . --settings ./settings.js flows.json\" } Let's take a closer look at the start command: node --max-old-space-size=256 (a) ./node_modules/node-red/red.js (b) --userDir . (c) --settings ./settings.js (d) flows.json (e) This argument is used to tell node when it should start garbage collecting. With node-red listed as an npm dependency of the project, we know exactly where it will get installed and where the red.js main entry point is. We want Node-RED to use the current directory as its user directory Just to be sure, we point at the settings file it should use - something we\u2019ll add in the next step Finally we specify the flow file to use. If you picked a different flow file name at the start, make sure you use the right name here. Having made those changes, restart Node-RED and reload the editor in your browser. Your complete package.json file should look something like this: { \"name\": \"photobooth-workshop\", \"description\": \"A Node-RED Project\", \"version\": \"0.0.1\", \"dependencies\": { \"node-red\": \"1.x\", \"node-red-contrib-tfjs-coco-ssd\": \"0.5.2\", \"node-red-dashboard\": \"2.23.5\", \"node-red-node-annotate-image\": \"0.1.0\", \"node-red-node-ui-table\": \"0.3.7\", \"node-red-node-ui-webcam\": \"0.2.1\" }, \"node-red\": { \"settings\": { \"flowFile\": \"flows.json\", \"credentialsFile\": \"flows_cred.json\" } }, \"scripts\": { \"start\": \"node --max-old-space-size=256 ./node_modules/node-red/red.js --userDir . --settings ./settings.js flows.json\" } } Next Steps \u00b6 With the package.js file updated, the next task is to add a settings.js file .","title":"Package file"},{"location":"part5/package.html#update-packagejson","text":"","title":"Update package.json"},{"location":"part5/package.html#adding-dependencies","text":"Through the workshop you've installed a number of additional modules into Node-RED. We need to make sure they are listed in the project's package.json file so they will get installed into the container. Open the Project Settings dialog from the main menu in the editor ( Projects -> Project Settings ) Switch to the Dependencies tab. You will see a list of the additional modules that are used by the project . Click the 'add to project' next to each one and then close the dialog. That will have updated the package.json file for you. However, for this scenario there's one more dependency we need to add manually - node-red itself. The package.json file can be found in ~/.node-red/projects/<name-of-project>/package.json . Open that file in a text editor and make the following changes: add node-red to the dependencies section - this will ensure Node-RED gets installed when the container is built. \"dependencies\": { \"node-red\": \"1.x\", ... }, Add a scripts section to define a start command - this is how the container will run Node-RED: \"scripts\": { \"start\": \"node --max-old-space-size=256 ./node_modules/node-red/red.js --userDir . --settings ./settings.js flows.json\" } Let's take a closer look at the start command: node --max-old-space-size=256 (a) ./node_modules/node-red/red.js (b) --userDir . (c) --settings ./settings.js (d) flows.json (e) This argument is used to tell node when it should start garbage collecting. With node-red listed as an npm dependency of the project, we know exactly where it will get installed and where the red.js main entry point is. We want Node-RED to use the current directory as its user directory Just to be sure, we point at the settings file it should use - something we\u2019ll add in the next step Finally we specify the flow file to use. If you picked a different flow file name at the start, make sure you use the right name here. Having made those changes, restart Node-RED and reload the editor in your browser. Your complete package.json file should look something like this: { \"name\": \"photobooth-workshop\", \"description\": \"A Node-RED Project\", \"version\": \"0.0.1\", \"dependencies\": { \"node-red\": \"1.x\", \"node-red-contrib-tfjs-coco-ssd\": \"0.5.2\", \"node-red-dashboard\": \"2.23.5\", \"node-red-node-annotate-image\": \"0.1.0\", \"node-red-node-ui-table\": \"0.3.7\", \"node-red-node-ui-webcam\": \"0.2.1\" }, \"node-red\": { \"settings\": { \"flowFile\": \"flows.json\", \"credentialsFile\": \"flows_cred.json\" } }, \"scripts\": { \"start\": \"node --max-old-space-size=256 ./node_modules/node-red/red.js --userDir . --settings ./settings.js flows.json\" } }","title":"Adding dependencies"},{"location":"part5/package.html#next-steps","text":"With the package.js file updated, the next task is to add a settings.js file .","title":"Next Steps"},{"location":"part5/settings.html","text":"Add a settings.js file \u00b6 You are already familiar with the Node-RED settings.js file you had to edit in the earlier part of the workshop. The containerized version of your application will need its own settings file to use. Create the file ~/.node-red/projects/<name-of-project>/settings.js with the following contents: module.exports = { uiPort: process.env.PORT || 1880, credentialSecret: process.env.NODE_RED_CREDENTIAL_SECRET, httpAdminRoot: false, ui: { path: \"/\" } } Setting httpAdminRoot to false will disable the Node-RED editor entirely - we do not want the flows to be edited directly in our production environment. credentialSecret is how you can provide the key for decrypting your credentials file. Rather than hardcode the key in the file - which is a bad idea - we set it using the environment variable NODE_RED_CREDENTIAL_SECRET . Having disabled the editor, the ui setting moves the root url of the dashboard page back to / rather than its default /ui . Next Steps \u00b6 The final task is to add a Dockerfile .","title":"Settings file"},{"location":"part5/settings.html#add-a-settingsjs-file","text":"You are already familiar with the Node-RED settings.js file you had to edit in the earlier part of the workshop. The containerized version of your application will need its own settings file to use. Create the file ~/.node-red/projects/<name-of-project>/settings.js with the following contents: module.exports = { uiPort: process.env.PORT || 1880, credentialSecret: process.env.NODE_RED_CREDENTIAL_SECRET, httpAdminRoot: false, ui: { path: \"/\" } } Setting httpAdminRoot to false will disable the Node-RED editor entirely - we do not want the flows to be edited directly in our production environment. credentialSecret is how you can provide the key for decrypting your credentials file. Rather than hardcode the key in the file - which is a bad idea - we set it using the environment variable NODE_RED_CREDENTIAL_SECRET . Having disabled the editor, the ui setting moves the root url of the dashboard page back to / rather than its default /ui .","title":"Add a settings.js file"},{"location":"part5/settings.html#next-steps","text":"The final task is to add a Dockerfile .","title":"Next Steps"},{"location":"part6/index.html","text":"Register and deploy the Pattern to the edge \u00b6 This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device. Instructions : Deploy Horizon Pattern","title":"Introduction"},{"location":"part6/index.html#register-and-deploy-the-pattern-to-the-edge","text":"This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device. Instructions : Deploy Horizon Pattern","title":"Register and deploy the Pattern to the edge"},{"location":"part6/HZNDEPLOY.html","text":"Register and deploy the Pattern to the edge \u00b6 This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device. Lab Objectives \u00b6 In this lab you will learn how to: Register the workload pattern on the Exchange server Deploy the pattern to the edge device \u00b6 Congratulations - You have completed this Code Pattern.","title":"Deploy Horizon Pattern"},{"location":"part6/HZNDEPLOY.html#register-and-deploy-the-pattern-to-the-edge","text":"This final section demonstrates how to register the workload pattern on the Exchange server and deploy the pattern to the edge device.","title":"Register and deploy the Pattern to the edge"},{"location":"part6/HZNDEPLOY.html#lab-objectives","text":"In this lab you will learn how to: Register the workload pattern on the Exchange server Deploy the pattern to the edge device","title":"Lab Objectives"},{"location":"part6/HZNDEPLOY.html#_1","text":"Congratulations - You have completed this Code Pattern.","title":""}]}